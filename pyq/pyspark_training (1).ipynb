{"cells": [{"cell_type": "code", "execution_count": 1, "id": "dd29c86c-f694-4fb3-aed4-924fd8652797", "metadata": {"tags": []}, "outputs": [], "source": "from google.cloud import storage"}, {"cell_type": "code", "execution_count": 2, "id": "62860abe-71aa-4c3f-bfe1-5b56dac2f533", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/08/24 07:14:08 INFO SparkEnv: Registering MapOutputTracker\n24/08/24 07:14:08 INFO SparkEnv: Registering BlockManagerMaster\n24/08/24 07:14:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n24/08/24 07:14:09 INFO SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, TimestampType, StringType\n\n\nschema = StructType([\n    StructField(\"random1\", StringType(), True),        \n    StructField(\"random2\", StringType(), True),\n    StructField(\"timestamp\", TimestampType(), True),\n    StructField(\"random4\", StringType(), True),\n    StructField(\"random5\", StringType(), True),\n    StructField(\"label\", DoubleType(), False),\n    StructField(\"reviews\", StringType(), True),\n    StructField(\"random8\", StringType(), True),\n    StructField(\"random9\", StringType(), True),\n])\n\nspark = SparkSession.builder.\\\n        master(\"local[*]\").\\\n        appName(\"processing\").\\\n        getOrCreate()\n"}, {"cell_type": "code", "execution_count": 3, "id": "19236021-76ab-4886-82b8-68cdd5f1be33", "metadata": {"tags": []}, "outputs": [], "source": "# myTable = spark.read.format(\"csv\").schema(schema).load(\"gs://pyq/training\")\nmyTable = spark.read.format(\"csv\").schema(schema).load(\"gs://pyq/output.csv\")\n"}, {"cell_type": "code", "execution_count": 4, "id": "f8d2fd0a-92ab-4f55-8a0e-f60970fb8df7", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+-------+---------+-------+-------+-----+-------+-------+-------+\n|random1|random2|timestamp|random4|random5|label|reviews|random8|random9|\n+-------+-------+---------+-------+-------+-----+-------+-------+-------+\n|      0|    283|      928|    649|    768|  911|    868|   1202|   1213|\n+-------+-------+---------+-------+-------+-----+-------+-------+-------+\n\n"}], "source": "from pyspark.sql import functions as F\n\n# Create a DataFrame with the count of null values for each column\nnull_counts = myTable.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in myTable.columns])\n\n# Show the result\nnull_counts.show()"}, {"cell_type": "code", "execution_count": 5, "id": "52e09677-0953-4421-8e4c-76a8dd145076", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+-------+-------------------+-------+--------------------+-----+--------------------+-------+--------------------+\n|             random1|random2|          timestamp|random4|             random5|label|             reviews|random8|             random9|\n+--------------------+-------+-------------------+-------+--------------------+-----+--------------------+-------+--------------------+\n|--FcbSxK1AoEtEAxO...|      0|2017-10-09 18:40:02|      0|-zF5IYw28q-ZOj8lh...|  5.0|Great place to go...|      0|dweQtsAEjtkpQQ_Tv...|\n+--------------------+-------+-------------------+-------+--------------------+-----+--------------------+-------+--------------------+\nonly showing top 1 row\n\n"}], "source": "myTable.show(1)"}, {"cell_type": "code", "execution_count": 6, "id": "50a31ca4-cf78-4d87-a9d1-870a965d30d8", "metadata": {"tags": []}, "outputs": [], "source": "columns_to_drop = ['random1', 'random2', 'random4', 'random5', 'random8', 'random9', 'random10']\nmyTable = myTable.drop(*columns_to_drop)"}, {"cell_type": "code", "execution_count": 7, "id": "368762cc-6e0d-4a5b-acbc-742180600e0a", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+-----+--------------------+\n|          timestamp|label|             reviews|\n+-------------------+-----+--------------------+\n|2017-10-09 18:40:02|  5.0|Great place to go...|\n+-------------------+-----+--------------------+\nonly showing top 1 row\n\n"}], "source": "myTable.show(1)"}, {"cell_type": "code", "execution_count": 8, "id": "42c24c6d-6cc3-4102-993d-57132ecfb5fe", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+-----+--------------------+\n|          timestamp|label|             reviews|\n+-------------------+-----+--------------------+\n|2017-10-09 18:40:02|  5.0|Great place to go...|\n|2018-05-13 13:12:18|  1.0|I used to come he...|\n|1970-01-01 00:00:00| NULL|                NULL|\n|1970-01-01 00:00:00| NULL|                NULL|\n|2018-11-08 19:35:49|  2.0|I purchased an $8...|\n|1970-01-01 00:00:00| NULL|                NULL|\n|1970-01-01 00:00:00| NULL|                NULL|\n|1970-01-01 00:00:00| NULL|                NULL|\n|1970-01-01 00:00:00| NULL|                NULL|\n|2019-01-14 18:47:26|  5.0|Free vacuuming af...|\n|1970-01-01 00:00:00| NULL|                NULL|\n|2019-03-13 20:24:45|  2.0|This car wash use...|\n|2019-03-30 14:38:10|  2.0|Very disappointed...|\n|2019-05-17 23:52:10|  2.0|This use to be on...|\n|2019-10-04 19:28:36|  5.0|With so many car ...|\n|2019-10-05 14:48:41|  5.0|Excellent attenti...|\n|2020-05-29 00:27:01|  5.0|Great experience ...|\n|2020-08-25 13:50:11|  1.0|Customer service ...|\n|2021-01-20 21:07:58|  1.0|This car wash dur...|\n|2019-04-18 21:14:07|  1.0|I never was able ...|\n+-------------------+-----+--------------------+\nonly showing top 20 rows\n\n"}], "source": "from pyspark.sql import functions as F\n\n# Convert timestamp to numeric (e.g., epoch time)\nmyTable = myTable.withColumn(\"timestamp_numeric\", F.col(\"timestamp\").cast(\"long\"))\n\n# Perform your operations on the numeric column\n# Example: fill missing values with a constant value or mode\nmyTable = myTable.na.fill({\"timestamp_numeric\": 0})\n\n# Convert back to timestamp\nmyTable = myTable.withColumn(\"timestamp\", F.col(\"timestamp_numeric\").cast(\"timestamp\")).drop(\"timestamp_numeric\")\n\n# Show the result\nmyTable.show()\n"}, {"cell_type": "code", "execution_count": 9, "id": "4315ee93-efd3-4458-aed7-6cd764e4e2db", "metadata": {"tags": []}, "outputs": [], "source": "# from pyspark.sql import functions as F\n\n# # Check for rows where the timestamp is 0 (or any other specific value you want to check)\n# zero_timestamp_rows = myTable.filter(F.col(\"timestamp\").cast(\"long\") == 0)\n\n# # Count the number of such rows\n# num_zero_timestamp_rows = zero_timestamp_rows.count()\n\n# # Show the rows with zero timestamps (optional)\n# zero_timestamp_rows.show()\n\n# print(f\"Number of rows with zero timestamp values: {num_zero_timestamp_rows}\")\n"}, {"cell_type": "code", "execution_count": 10, "id": "38b18998-e26d-4aaa-80be-d0359403def8", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+-----+--------------------+\n|          timestamp|label|             reviews|\n+-------------------+-----+--------------------+\n|2017-10-09 18:40:02|  5.0|Great place to go...|\n|2018-05-13 13:12:18|  1.0|I used to come he...|\n|1970-01-01 00:00:00| NULL|                NULL|\n|1970-01-01 00:00:00| NULL|                NULL|\n|2018-11-08 19:35:49|  2.0|I purchased an $8...|\n|1970-01-01 00:00:00| NULL|                NULL|\n|1970-01-01 00:00:00| NULL|                NULL|\n|1970-01-01 00:00:00| NULL|                NULL|\n|1970-01-01 00:00:00| NULL|                NULL|\n|2019-01-14 18:47:26|  5.0|Free vacuuming af...|\n|1970-01-01 00:00:00| NULL|                NULL|\n|2019-03-13 20:24:45|  2.0|This car wash use...|\n|2019-03-30 14:38:10|  2.0|Very disappointed...|\n|2019-05-17 23:52:10|  2.0|This use to be on...|\n|2019-10-04 19:28:36|  5.0|With so many car ...|\n|2019-10-05 14:48:41|  5.0|Excellent attenti...|\n|2020-05-29 00:27:01|  5.0|Great experience ...|\n|2020-08-25 13:50:11|  1.0|Customer service ...|\n|2021-01-20 21:07:58|  1.0|This car wash dur...|\n|2019-04-18 21:14:07|  1.0|I never was able ...|\n+-------------------+-----+--------------------+\nonly showing top 20 rows\n\n"}], "source": "from pyspark.ml.feature import Imputer\nfrom pyspark.sql import functions as F\n\n# Convert timestamp to numeric (e.g., epoch time)\nmyTable = myTable.withColumn(\"timestamp_numeric\", F.col(\"timestamp\").cast(\"long\"))\n\n# Initialize the Imputer\nimputer = Imputer(inputCols=[\"timestamp_numeric\"], outputCols=[\"timestamp_numeric\"])\n\n# Fit the Imputer model and transform the DataFrame\nimputed_df = imputer.fit(myTable).transform(myTable)\n\n# Convert back to timestamp\nimputed_df = imputed_df.withColumn(\"timestamp\", F.col(\"timestamp_numeric\").cast(\"timestamp\")).drop(\"timestamp_numeric\")\n\n# Show the result\nimputed_df.show()\n"}, {"cell_type": "code", "execution_count": 11, "id": "9827236d-1465-4e5b-a99d-4fba63df0e5a", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+\n|null_count|\n+----------+\n|         0|\n+----------+\n\n"}], "source": "# Fill null values in 'label' with a default value (e.g., 0.0)\nmyTable = myTable.na.fill({\"label\": 0.0})\n\n# Verify that nulls have been filled\nmyTable.select(F.sum(F.col(\"label\").isNull().cast(\"int\")).alias(\"null_count\")).show()\n"}, {"cell_type": "code", "execution_count": 12, "id": "108cb313-763c-4a93-9cfe-e89259a5ae64", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+\n|null_count|\n+----------+\n|         0|\n+----------+\n\n"}], "source": "# Fill null values in 'label' with a default value (e.g., 0.0)\nmyTable = myTable.na.fill({\"reviews\": ''})\n\n# Verify that nulls have been filled\nmyTable.select(F.sum(F.col(\"reviews\").isNull().cast(\"int\")).alias(\"null_count\")).show()"}, {"cell_type": "code", "execution_count": 13, "id": "97057f5f-c6cf-4afb-a367-ed76acaeeba1", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+-----+-------+-----------------+\n|timestamp|label|reviews|timestamp_numeric|\n+---------+-----+-------+-----------------+\n|        0|    0|      0|                0|\n+---------+-----+-------+-----------------+\n\n"}], "source": "from pyspark.sql import functions as F\n\n# Create a DataFrame with the count of null values for each column\nnull_counts = myTable.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in myTable.columns])\n\n# Show the result\nnull_counts.show()\n"}, {"cell_type": "code", "execution_count": 14, "id": "1802bc37-275c-42a6-b604-dd6e00f158ef", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+-----+--------------------+-----------------+\n|          timestamp|label|             reviews|timestamp_numeric|\n+-------------------+-----+--------------------+-----------------+\n|2017-10-09 18:40:02|  5.0|Great place to go...|       1507574402|\n|2018-05-13 13:12:18|  1.0|I used to come he...|       1526217138|\n|1970-01-01 00:00:00|  0.0|                    |                0|\n|1970-01-01 00:00:00|  0.0|                    |                0|\n|2018-11-08 19:35:49|  2.0|I purchased an $8...|       1541705749|\n|1970-01-01 00:00:00|  0.0|                    |                0|\n|1970-01-01 00:00:00|  0.0|                    |                0|\n|1970-01-01 00:00:00|  0.0|                    |                0|\n|1970-01-01 00:00:00|  0.0|                    |                0|\n|2019-01-14 18:47:26|  5.0|Free vacuuming af...|       1547491646|\n|1970-01-01 00:00:00|  0.0|                    |                0|\n|2019-03-13 20:24:45|  2.0|This car wash use...|       1552508685|\n|2019-03-30 14:38:10|  2.0|Very disappointed...|       1553956690|\n|2019-05-17 23:52:10|  2.0|This use to be on...|       1558137130|\n|2019-10-04 19:28:36|  5.0|With so many car ...|       1570217316|\n|2019-10-05 14:48:41|  5.0|Excellent attenti...|       1570286921|\n|2020-05-29 00:27:01|  5.0|Great experience ...|       1590712021|\n|2020-08-25 13:50:11|  1.0|Customer service ...|       1598363411|\n|2021-01-20 21:07:58|  1.0|This car wash dur...|       1611176878|\n|2019-04-18 21:14:07|  1.0|I never was able ...|       1555622047|\n+-------------------+-----+--------------------+-----------------+\nonly showing top 20 rows\n\n"}], "source": "myTable.show()"}, {"cell_type": "code", "execution_count": 15, "id": "df3a02ae-7f80-4540-8430-b848b753974e", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Original number of rows: 1848\n"}], "source": "original_count = myTable.count()\nprint(f\"Original number of rows: {original_count}\")"}, {"cell_type": "code", "execution_count": 16, "id": "5702a690-9e86-4b1d-bf10-dc0a8b3075bb", "metadata": {"tags": []}, "outputs": [], "source": "# from pyspark.ml.feature import StringIndexer, OneHotEncoder, Tokenizer\n# from pyspark.sql.functions import explode\n\n# # Step 1: Tokenize the text\n# tokenizer = Tokenizer(inputCol=\"reviews\", outputCol=\"words\")\n# wordsData = tokenizer.transform(myTable)\n\n# # # Step 2: Explode tokens into separate rows\n# explodedWordsData = wordsData.withColumn(\"word\", explode(wordsData.words))\n\n# # Step 3: Filter out empty tokens if any\n# filteredWordsData = explodedWordsData.filter(explodedWordsData.word != \"\")\n\n# # Step 4: Index the words\n# indexer = StringIndexer(inputCol=\"word\", outputCol=\"word_index\")\n# indexedData = indexer.fit(filteredWordsData).transform(filteredWordsData)\n\n# # Step 5: One-hot encode the indexed words\n# encoder = OneHotEncoder(inputCols=[\"word_index\"], outputCols=[\"onehot_features\"])\n# oneHotData = encoder.fit(indexedData).transform(indexedData)\n\n# # Step 6: Show the result\n# oneHotData.select(\"reviews\", \"word\", \"onehot_features\").show(truncate=False)\n"}, {"cell_type": "code", "execution_count": 17, "id": "3397b671-6d5c-4359-8233-7a3467bf292c", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, Tokenizer\nfrom pyspark.ml.feature import SQLTransformer\n\n# Step 1: Tokenize the text\ntokenizer = Tokenizer(inputCol=\"reviews\", outputCol=\"words\")\n\n# Step 2: Explode tokens into separate rows using SQLTransformer\nexplode_transformer = SQLTransformer(\n    statement=\"SELECT *, EXPLODE(words) AS word FROM __THIS__\"\n)\n\n# Step 3: Filter out empty tokens using SQLTransformer\nfilter_transformer = SQLTransformer(\n    statement=\"SELECT * FROM __THIS__ WHERE word != ''\"\n)\n\n# Step 4: Index the words\nindexer = StringIndexer(inputCol=\"word\", outputCol=\"word_index\")\n\n# Step 5: One-hot encode the indexed words\nencoder = OneHotEncoder(inputCols=[\"word_index\"], outputCols=[\"onehot_features\"])\n\n# Assemble the stages into a pipeline\npipeline = Pipeline(stages=[tokenizer, explode_transformer, filter_transformer, indexer, encoder])\n"}, {"cell_type": "code", "execution_count": 18, "id": "94040899-bdfb-4e90-b61e-6a5800a25452", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Fit the pipeline model\npipeline_model = pipeline.fit(myTable)\n"}, {"cell_type": "code", "execution_count": 19, "id": "0cf0da1b-af85-4ab6-8251-78693dfbc8d3", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------------+\n|reviews                                                                                                                                                                                |word |onehot_features  |\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------------+\n|Great place to go get your car washed took out the monthly package the guys there are really helpful and go beyond to help you suggest using their car wash really nice friendly people|great|(8445,[30],[1.0])|\n|Great place to go get your car washed took out the monthly package the guys there are really helpful and go beyond to help you suggest using their car wash really nice friendly people|place|(8445,[33],[1.0])|\n|Great place to go get your car washed took out the monthly package the guys there are really helpful and go beyond to help you suggest using their car wash really nice friendly people|to   |(8445,[4],[1.0]) |\n|Great place to go get your car washed took out the monthly package the guys there are really helpful and go beyond to help you suggest using their car wash really nice friendly people|go   |(8445,[57],[1.0])|\n|Great place to go get your car washed took out the monthly package the guys there are really helpful and go beyond to help you suggest using their car wash really nice friendly people|get  |(8445,[53],[1.0])|\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------------+\nonly showing top 5 rows\n\n"}], "source": "# Apply the pipeline model to the DataFrame\ntransformed_data = pipeline_model.transform(myTable)\n\n# Select specific columns and show the results\ntransformed_data.select(\"reviews\", \"word\", \"onehot_features\").show(5, truncate=False)\n"}, {"cell_type": "code", "execution_count": 20, "id": "ec03909b-8a65-4587-a7cc-74763228173c", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Save the entire pipeline model\npipeline_model_path = \"gs://pyq/pipeline_model\"\npipeline_model.write().overwrite().save(pipeline_model_path)\n"}, {"cell_type": "code", "execution_count": 21, "id": "f324ebe3-94de-451b-b361-fb8f9457b499", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/08/24 07:15:10 WARN DAGScheduler: Broadcasting large task binary with size 1309.1 KiB\n24/08/24 07:15:21 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:25 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:26 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:26 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:27 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:28 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:28 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:29 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:29 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:30 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:31 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:31 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:32 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:32 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:33 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:34 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:34 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:35 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:35 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:36 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:36 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:37 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:37 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:38 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:38 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:39 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:39 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:40 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:40 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:41 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:42 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:42 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:43 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:43 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:44 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:44 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:45 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:45 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:46 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:46 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:47 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:48 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:48 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:49 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:49 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:49 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:50 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:50 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:51 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:51 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:52 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:52 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:53 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:53 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:54 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:54 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:54 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:55 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:56 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:56 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:57 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:57 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:57 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:58 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:58 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:59 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:15:59 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:00 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:00 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:01 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:01 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:02 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:02 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:03 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:03 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:04 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:04 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:05 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:05 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:05 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:06 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:06 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:07 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:07 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:07 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:08 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:08 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:09 WARN DAGScheduler: Broadcasting large task binary with size 1309.9 KiB\n24/08/24 07:16:09 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n         \njava.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:121)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1676)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1661)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1580)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1397)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1332)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2991)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1397)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1332)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2991)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2415)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2510)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1199)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1193)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1286)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1253)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1239)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1239)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\n\tat breeze.optimize.LineSearch$$anon$1.calculate(LineSearch.scala:52)\n\tat breeze.optimize.LineSearch$$anon$1.calculate(LineSearch.scala:31)\n\tat breeze.optimize.StrongWolfeLineSearch.phi$1(StrongWolfe.scala:76)\n\tat breeze.optimize.StrongWolfeLineSearch.$anonfun$minimizeWithBound$2(StrongWolfe.scala:110)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n\tat breeze.optimize.StrongWolfeLineSearch.zoom$1(StrongWolfe.scala:105)\n\tat breeze.optimize.StrongWolfeLineSearch.$anonfun$minimizeWithBound$7(StrongWolfe.scala:165)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n\tat breeze.optimize.StrongWolfeLineSearch.minimizeWithBound(StrongWolfe.scala:151)\n\tat breeze.optimize.StrongWolfeLineSearch.minimize(StrongWolfe.scala:62)\n\tat breeze.optimize.LBFGS.determineStepSize(LBFGS.scala:82)\n\tat breeze.optimize.LBFGS.determineStepSize(LBFGS.scala:38)\n\tat breeze.optimize.FirstOrderMinimizer.$anonfun$infiniteIterations$1(FirstOrderMinimizer.scala:63)\n\tat scala.collection.Iterator$$anon$7.next(Iterator.scala:140)\n\tat breeze.util.IteratorImplicits$RichIterator$$anon$2.next(Implicits.scala:79)\n\tat org.apache.spark.ml.classification.LogisticRegression.trainImpl(LogisticRegression.scala:1015)\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:634)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:121)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1676)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1661)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1580)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1397)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1332)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2991)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\nERROR:root:Exception while sending command.\nTraceback (most recent call last):\n  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_23416/3726293178.py\", line 16, in <module>\n    lr_model = lr.fit(train_df)\n               ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/pyspark/ml/base.py\", line 205, in fit\n    return self._fit(dataset)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/pyspark/ml/wrapper.py\", line 381, in _fit\n    java_model = self._fit_java(dataset)\n                 ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/pyspark/ml/wrapper.py\", line 378, in _fit_java\n    return self._java_obj.fit(dataset._jdf)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: <exception str() failed>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n    raise Py4JNetworkError(\"Answer from Java side is empty\")\npy4j.protocol.Py4JNetworkError: Answer from Java side is empty\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n    response = connection.send_command(command)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n    raise Py4JNetworkError(\npy4j.protocol.Py4JNetworkError: Error while sending or receiving\n"}, {"ename": "ConnectionRefusedError", "evalue": "[Errno 111] Connection refused", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n", "Cell \u001b[0;32mIn[21], line 16\u001b[0m\n\u001b[1;32m     15\u001b[0m lr \u001b[38;5;241m=\u001b[39m LogisticRegression(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n", "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))", "\nDuring handling of the above exception, another exception occurred:\n", "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)", "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2179\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2176\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n", "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"]}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, Tokenizer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.sql.functions import explode\n\n# Assemble features\nassembler = VectorAssembler(inputCols=[\"onehot_features\", \"timestamp_numeric\"], outputCol=\"features\")\nassembled_df = assembler.transform(transformed_data)\n\n# Split data\ntrain_df, test_df = assembled_df.randomSplit([0.8, 0.2], seed=1234)\n\n# Train the model\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\nlr_model = lr.fit(train_df)\n\n# Make predictions\npredictions = lr_model.transform(test_df)\n\n# Evaluate the model\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(f\"Accuracy: {accuracy}\")\n\n# Stop Spark session\n# spark.stop()\n"}, {"cell_type": "code", "execution_count": null, "id": "28767364-6254-45e1-a302-da31561c2be6", "metadata": {"tags": []}, "outputs": [], "source": "lr_model.save(\"gs://pyq/lrmodel\")"}, {"cell_type": "code", "execution_count": null, "id": "801fb02a-14a4-4b51-83c0-72fe47abde4a", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.ml.classification import LogisticRegressionModel\n\n# Load the saved model\nmodel_path = \"gs://pyq/lrmodel\"\nmodel = LogisticRegressionModel.load(model_path)\n\n# Use the model to make predictions\npredictions = model.transform(test_df)\naccuracy = evaluator.evaluate(predictions)\nprint(f\"Accuracy: {accuracy}\")\n"}, {"cell_type": "code", "execution_count": null, "id": "9ea828d9-454e-48a5-aff9-9046fea71e87", "metadata": {"tags": []}, "outputs": [], "source": "accuracy"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}